#!/usr/bin/env bash
set -euo pipefail

# Convenience wrapper for running PySpark against the local-data managed
# Hadoop/Hive environment (config overlay, env vars, etc).
#
# Usage:
#   pyspark-b
#   pyspark-b --conf spark.sql.shuffle.partitions=4

script_dir="$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" && pwd)"
local_data="$script_dir/local-data"

exec "$local_data" env exec -- pyspark "$@"
